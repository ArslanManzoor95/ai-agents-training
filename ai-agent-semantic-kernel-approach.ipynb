{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58187f4",
   "metadata": {},
   "source": [
    "# Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ab5c4",
   "metadata": {},
   "source": [
    "Example code. Let's look at examples of how you can use a pre-built AI Connector with Semantic Kernel Python and .Net that uses auto-function calling to have the model respond to user input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Kernel Python Example\n",
    "\n",
    "import asyncio # library for asynchronous programming (non-blocking tasks) = improve efficiency\n",
    "# normally python runs code line by line and waits for each step to finish before moving on = BLOCKING \n",
    "# async useful - when callng external services like azure oienai, program might wait for response for few seconds - if we block during that time, nothing else can run\n",
    "# async solves this by letting program do other work while waiting for response \n",
    "from typing import Annotated # adds metadata to type hints (used for function argument descriptions)\n",
    "\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureChatPromptExecutionSettings # connector to azure openai\n",
    "from semantic_kernel.contents import ChatHistory # stores conversation context\n",
    "from semantic_kernel.functions import kernel_function # decorator that marks a method (function in a class) as callable by the LLM\n",
    "from semantic_kernel.kernel import Kernel # core orchestrator that manages plugins and AI connectors\n",
    "\n",
    "# Define a ChatHistory object to hold the conversation's context\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_user_message(\"I'd like to go to New York on January 1, 2025\")\n",
    "\n",
    "\n",
    "# Define a sample plugin that contains the function (method) to book travel\n",
    "class BookTravelPlugin:\n",
    "    \"\"\"A Sample Book Travel Plugin\"\"\"\n",
    "\n",
    "    @kernel_function(name=\"book_flight\", description=\"Book travel given location and date\") # this decorator tells semantic kernel this function can be called by LLM (registers the method (book_flight) as a callable tool for LLM)\n",
    "    async def book_flight( # async def makes this function asynchronous so it doesn't block other tasks\n",
    "        self, date: Annotated[str, \"The date of travel\"], location: Annotated[str, \"The location to travel to\"] #Adds descriptions for arguments (helps the LLM know what to supply).\n",
    "    ) -> str: # annotated above - LLM uses the descrpitions to know what arguments it needs to supply when calling the function\n",
    "        return f\"Travel was booked to {location} on {date}\" # returns a confirmation string \"mock\" - in real app, replace with an API call to airline system  + database update + email conf.\n",
    "    \n",
    "    # self is always first argument in class method -- refers to instance of the class (so method can access class attributes)\n",
    "    # date and location are the actual args passed by the LLM when it calls the function\n",
    "\n",
    "# Create the Kernel\n",
    "kernel = Kernel() # creates a kernel instance\n",
    "\n",
    "# Add the sample plugin to the Kernel object\n",
    "# plugin is just a class with 1+ functions that the LLM can call\n",
    "kernel.add_plugin(BookTravelPlugin(), plugin_name=\"book_travel\") # registers the plugin\n",
    "# after registering, the LLM knows: plugin name (book_travel) and functions inside it (book_flight)\n",
    "\n",
    "# not all fucntions need to be in same class - you can put multiple related functions in one class, or diff classes for diff domains\n",
    "# each class becomes separate plugin when registered\n",
    "\n",
    "# Define the Azure OpenAI AI Connector - this connects to azure openai for chat completions\n",
    "chat_service = AzureChatCompletion(\n",
    "    deployment_name=\"YOUR_DEPLOYMENT_NAME\", \n",
    "    api_key=\"YOUR_API_KEY\", \n",
    "    endpoint=\"https://<your-resource>.azure.openai.com/\",\n",
    ")\n",
    "\n",
    "# Define the request settings to configure the model with auto-function calling\n",
    "request_settings = AzureChatPromptExecutionSettings(function_choice_behavior=FunctionChoiceBehavior.Auto()) # lets model automatically decide when to call our function\n",
    "\n",
    "async def main(): # main is entry point for async workflow\n",
    "    # Make the request to the model for the given chat history and request settings\n",
    "    # The Kernel contains the sample that the model will request to invoke\n",
    "    response = await chat_service.get_chat_message_content( # await waits for the async call to finish \"pause here until result comes back, but don't freeze everything else\"\n",
    "        chat_history=chat_history, settings=request_settings, kernel=kernel # passes the chat history, settings, and kernel (with plugins) to the model\n",
    "    )\n",
    "    \n",
    "    assert response is not None\n",
    "\n",
    "    \"\"\"\n",
    "    Note: In the auto function calling process, the model determines it can invoke the \n",
    "    `BookTravelPlugin` using the `book_flight` function, supplying the necessary arguments. \n",
    "    \n",
    "    For example:\n",
    "\n",
    "    \"tool_calls\": [\n",
    "        {\n",
    "            \"id\": \"call_abc123\",\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"BookTravelPlugin-book_flight\",\n",
    "                \"arguments\": \"{'location': 'New York', 'date': '2025-01-01'}\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    Since the location and date arguments are required (as defined by the kernel function), if the \n",
    "    model lacks either, it will prompt the user to provide them. For instance:\n",
    "\n",
    "    User: Book me a flight to New York.\n",
    "    Model: Sure, I'd love to help you book a flight. Could you please specify the date?\n",
    "    User: I want to travel on January 1, 2025.\n",
    "    Model: Your flight to New York on January 1, 2025, has been successfully booked. Safe travels!\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"`{response}`\")\n",
    "    # Example AI Model Response: `Your flight to New York on January 1, 2025, has been successfully booked. Safe travels! ‚úàÔ∏èüóΩ`\n",
    "    \n",
    "\n",
    "    # Add the model's response to our chat history context\n",
    "    chat_history.add_assistant_message(response.content)\n",
    "\n",
    "# the above sends chat history and plugin info to model, which decides if needs to call book_flight\n",
    "# model returns a response e.g. your glight is booked - which we print and add to chat history\n",
    "\n",
    "if __name__ == \"__main__\": # runs the async main() function in an event loop\n",
    "    asyncio.run(main())\n",
    "\n",
    "# Summary: \n",
    "# Kernel = Orchestrator.\n",
    "# Plugin = Your custom tools.\n",
    "# LLM = Decides when to call tools.\n",
    "# Async = Makes network calls efficient.\n",
    "# Annotated = Helps LLM understand arguments.\n",
    "# @kernel_function = Registers the function for auto-calling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b628ab",
   "metadata": {},
   "source": [
    "Example code (AutoGen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424fb87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating agents, then create a round robin schedule where they can work together, in this case in order\n",
    "\n",
    "# Data Retrieval Agent --> fetches data using a tool\n",
    "# Data Analysis Agent --> analyses data using another tool\n",
    "# Decision Making Agent --> represents the human user\n",
    "\n",
    "# Then it sets up a Round Robin schedule so these agents can collaborate in turns until a termination condition is met (user says \"APPROVE\").\n",
    "# Finally, it runs the conversation as a stream.\n",
    "\n",
    "### What is Happening Conceptually\n",
    "\n",
    "# 1. User says ‚ÄúAnalyze data‚Äù.\n",
    "# 2. Retrieve agent fetches data using retrieve_tool.\n",
    "# 3. Analyze agent processes data using analyze_tool.\n",
    "# 4. User proxy can approve or give feedback.\n",
    "# 5. Repeat until user says \"APPROVE\" or max turns reached.\n",
    "\n",
    "###Do all tools need to be in one class?\n",
    "# No. Tools can be standalone functions or grouped in plugins. Agents can have multiple tools.\n",
    "\n",
    "agent_retrieve = AssistantAgent( # AssistantAgent: A class that wraps an LLM and gives it a role.\n",
    "    name=\"dataretrieval\", # identifier for the agent\n",
    "    model_client=model_client, # The LLM backend (e.g., OpenAI, Azure).\n",
    "    tools=[retrieve_tool], # Functions the agent can call (here, retrieve_tool).\n",
    "    system_message=\"Use tools to solve tasks.\" # Defines the agent‚Äôs behavior (like a role prompt).\n",
    ")\n",
    "\n",
    "agent_analyze = AssistantAgent(\n",
    "    name=\"dataanalysis\",\n",
    "    model_client=model_client,\n",
    "    tools=[analyze_tool],\n",
    "    system_message=\"Use tools to solve tasks.\"\n",
    ")\n",
    "\n",
    "# conversation ends when user says \"APPROVE\"\n",
    "termination = TextMentionTermination(\"APPROVE\") # Conversation stops when any message contains ‚ÄúAPPROVE‚Äù.This is useful for controlled workflows.\n",
    "\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input) # Represents the human user. input_func=input means it will read user input from the console.\n",
    "\n",
    "team = RoundRobinGroupChat([agent_retrieve, agent_analyze, user_proxy], termination_condition=termination)\n",
    "# above creates a team of agents\n",
    "# round robin means each agent takes turns in a fixed order. e.g., retrieve --> analyse --> user --> retrieve --> analyse --> ...\n",
    "# stops when termination condition is met\n",
    "\n",
    "stream = team.run_stream(task=\"Analyze data\", max_turns=10) # run_stream strarts the convo, initial task is analyse data, max turns is 10\n",
    "# Use asyncio.run(...) when running in a script.\n",
    "await Console(stream) # await because this is async (non-blocking) - async lets multiple agents work efficiently without blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088cef63",
   "metadata": {},
   "source": [
    "Here you have a short code snippet in which you create your own agent with Chat capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121867b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent # AssistantAgent: A ready-made agent that can handle chat completions.\n",
    "from autogen_agentchat.messages import TextMessage # TextMessage: Represents a message in the conversation.\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient # OpenAIChatCompletionClient: Connects to OpenAI‚Äôs GPT models.\n",
    "\n",
    "# This code shows how to:\n",
    "\n",
    "# 1. Create a custom agent (MyAgent) that can handle messages.\n",
    "# 2. Use AutoGen‚Äôs runtime to register the agent and send messages.\n",
    "# 3. Delegate actual chat handling to a pre-built AssistantAgent (which knows how to talk to an LLM like GPT-4).\n",
    "\n",
    "# MyAgent ‚Üí AssistantAgent ‚Üí GPT-4 ‚Üí Response\n",
    "\n",
    "\n",
    "class MyAgent(RoutedAgent): # MyAgent inherits from RoutedAgent (a base class for agents that can route messages).\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name) # to initialize the parent class.\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") # Creates model_client for GPT-4.\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client) # Assigns self._delegate = an AssistantAgent instance.\n",
    "        # This means MyAgent doesn‚Äôt handle chat logic itself‚Äîit delegates to AssistantAgent.\n",
    "\n",
    "    @message_handler #  A decorator that marks this method as a handler for a specific message type.\n",
    "    async def handle_my_message_type(self, message: MyMessageType, ctx: MessageContext) -> None: # async def: Because message handling involves calling the LLM (network I/O).\n",
    "        print(f\"{self.id.type} received message: {message.content}\") # Print the incoming message.\n",
    "        response = await self._delegate.on_messages( # Call self._delegate.on_messages(...): Converts the message into a TextMessage. Passes it to the AssistantAgent (which talks to GPT-4).\n",
    "            [TextMessage(content=message.content, source=\"user\")], ctx.cancellation_token\n",
    "        )\n",
    "        print(f\"{self.id.type} responded: {response.chat_message.content}\") # Print the LLM‚Äôs response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81017806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# main.py\n",
    "runtime = SingleThreadedAgentRuntime() # Runs agents in a single thread.Runtime Orchestrates agents and message passing.\n",
    "await MyAgent.register(runtime, \"my_agent\", lambda: MyAgent())  # Registers your custom agent with the runtime. \"my_agent\" = agent name. lambda: MyAgent() = factory function to create the agent.\n",
    "# Agent registration: Tells AutoGen what agents exist and how to create them.\n",
    "\n",
    "# Here, runtime.start() and send_message() kick off the event loop for message processing\n",
    "runtime.start()  # Start processing messages in the background.\n",
    "await runtime.send_message(MyMessageType(\"Hello, World!\"), AgentId(\"my_agent\", \"default\")) # Sends \"Hello, World!\" to my_agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616706d0",
   "metadata": {},
   "source": [
    "Output:\n",
    "<br>my_agent received message: Hello, World!\n",
    "<br>my_assistant received message: Hello, World!\n",
    "<br>my_assistant responded: Hello! How can I assist you today?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fae50d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
